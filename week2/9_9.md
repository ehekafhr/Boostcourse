# 정리


## 아인슈타인 표기법

강의에 나온 함수들 뿐만 아니라, 좀더 많은 함수들도 알아보자.

차원이 높아짐에 따라.. `dot()`이나, `matmul()`함수가 어떻게 동작할지는 생각하기 어렵다.

텐서의 연산을 조금 더 직관적으로 표현하기 위해서는, `numpy.einsum`, `einops`를 사용하는 게 좋다.

`numpy.einsum`은 index의 변화를 string으로 받는다.

직관적으로, 우리가 사용하는 행렬곱은 'ik, kj -> ij'의 차원 변화를 갖는다.

이를 그대로 `numpy.einsum()`에 넣고, 두 행렬을 넣으면 `k`차원 벡터끼리의 내적을 계산해 행렬곱을 진행해 준다.

만약 'ij -> ji'식으로 하나의 행렬에 대한 식을 적으면, i와 j를 뒤집어 준다. 차원이 같아 하나의 값에 대한 sum을 하기 때문에 값이 유지되어, 행과 열만 바뀌는 것으로 보인다.

이떄, 'ii'등을 사용하면 행과 열이 같은 값들만 가져오는 식으로, trace 값들을 구할 수 있다.

'bii -> b'를 사용하면 b차원의 ii, 즉 대각성분만 더하는 식으로 작동한다.

`einops` 알아보기

`einops`는 `einsum`과 비슷하지만 조금 더 많은 함수들을 가지고 있다.

https://einops.rocks/ 에서 찾아보면..

`einops.rearrange`는 `view`나 `reshape`와 같지만, 차원을 알 필요가 없다.

`transpose`처럼, 차원의 순서만 알고 있으면 그것을 묶는 것이 가능하다.

묶는 것 뿐만 아니라 크기 8의 차원 b 을 4,2로 분리하려면, '(b db) -> b db', db = 2

이런 식으로 할 수 있다. 다만 이 경우에는 db로 b가 나누어진다는 것을 알아야 한다.

`repeat()`은 원하는 축에 대해 '(tile 축)', tile = 횟수 식으로,

`reduce()` 는 함수와 합칠 축을 찾아, 각각의 축에 함수를 적용시켜 텐서를 축소시킨다.

`reduce(A, '(x dx) (y dy) (z dz) -> x y z', 'min', dx=3, dy=4, dz=5)` 를 하면

A를 dx = 3, dy = 4, dz = 5인 block으로 나누어 min값을 넣은 tensor를 반환해 준다.

`np.einsum`외에도 `einops.einsum`이 있는 것 같다. 아마 같은 동작인듯 싶다.

과제 2번은 과제보다는 실습에 가까웠다.

주의할 점은, `einops`는 `np`에 있는 게 아니라 따로 `import`해야 하고,

`np.einsum()`과 다르게 각각의 변수를 띄워줘야 한다는 것이다.

`'b i j k'`이런 식으로.. 아마 붙어있는 것들을 하나로 취급하는 듯 하다.

## Gradient Descent

### 미분

일변수 함수 $y = f(x)$ 의 경우에는 단순히 x에 대한 미분값을 기울기로 해석할 수 있다.

이때, 기울기 부호의 방향으로 $x$가 움직이면 $f(x)$ 가 증가하고, 반대면 감소한다. 움직이는 크기는 $x \times f'(x)$ 로 볼 수 있지만, $x$ 가 커질수록 부정확해진다.

이를 다변수로 확장시키면,  **x** = $[x_1,x_2,x_3...,x_m]$ 에 대해 각각의 편미분값 $\partial_{x_i}$를 구하면

$\triangledown$ f(**x**) = $[\partial_{x_1},\partial_{x_2},\partial_{x_3},...,\partial_{x_m}]$ 로 구할 수 있다.

이 벡터에 **x**를 대입하면 하나의 방향 벡터가 나오는데, 이 벡터의 방향은 m차원 공간 상에서 목적함수가 가장 빠르게 증가하는 방향벡터를 의미한다. 크기는 얼마나 빠르게 증가하는지이다.

따라서, 여기에 -1을 곱해주면 가장 빠르게 감소하는 방향이다(하나의 m차원 hyperplane을 생각하면 그 반대 방향 벡터도 있음을 짐작할 수 있다)

### 알고리즘

기본적인 경사하강법은 학습률 learning rate lr, 종료조건 $\epsilon$ eps를 인자로 받는다.

learning rate는 학습이 진행되는 속도로, grad에 곱해지는 값이다. 

eps는 최대 오차로, 오차값이 이보다 작아지면 학습이 충분히 진행되었다고 생각하고 멈춘다.

의사코드를 적어보면

```
lr = 0.01, eps = 1e-5 #hyperparameter이므로, 알아서 설정

result = random #만약 공간이 convex하지 않다면, 이 초깃값에 따라 local minima에 빠질지가 결정된다.
while(grad.norm() < eps):
    grad = gradient(result)
    result -= lr * grad 
```

### 선형회귀에서의 적용

선형회귀에서의 목적식은

$$ \Vert \mathbf{y} - \mathbf{X} \beta \Vert_2 $$ 

이고ㅛ, 여기서 $\beta$ 를 찾는 것이 목적이다.

Gradient descent를 사용하기 전에, 이 목적식은 root를 취하므로 계산을 간단하게 하기 위해 제곱을 해서 사용한다.

norm이 음수가 아니기 때문에, 제곱한 값을 최소화하는 것과 원래 값을 최소화하는 것은 동치이다.

(RMSE를 최적화하는 대신, MSE를 최적화)

$$ \triangledown_{\beta} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2} = (\partial_{\beta_1}  \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2}, ... , \partial_{\beta_d} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2}) $$

를 구하면 된다. 

$y = f(x)$ 일 때, $y^2$ 를 x에 대해 미분하면 $2y'y$ 가 나온다. 위의 식에서 y는 $\mathbf{y} - \mathbf{X} \beta_i$ 가 된다. 왼쪽의 y는 x인 $\beta_i$ 와 무관하므로 오른쪽의 미분값만 계산하면

$\beta_i$가 곱해지는 부분은 $\mathbf{X}$ 의 $i$ 번째 열 뿐이므로, y'은 $-2\mathbf{X}_{i}^{T}$ 가 된다.

따라서 $(\partial_{\beta_i} =  -2\mathbf{X}_{i}^{T}(\mathbf(y)-\mathbf{X}\beta)$ 로 구할 수 있고, 이를 이어붙히면

$$ \triangledown_{\beta} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2} = -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) $$ 

가 된다.

하지만 이 값은 데이터가 많아질수록 커지는 특성을 가지고 있기 때문에, SSE 대신 MSE를 사용하기 위해 데이터의 갯수 n으로 나누어 준다.

$$ \frac{1}{n} \triangledown_{\beta} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2} = -\frac{2}{n}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) $$ 

이 식을 gradient descent에 넣어서 활용한다.

$$ \beta^{(t+1)} = \beta{(t)} + \frac{2 * \lambda}{n}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) $$

이때 $2*\lambda$ 에서 2는 어차피 고정된 값이므로 이 자체를 learning rate로 설정한다.

gradient를 계산하는 부분을

`grad = - X.T @ (y - X @ beta)`

로 바꾸어 주면 된다.

또한 선형 회귀에서는 목적함수를 최소화하지만, 목적함수가 얼마나 작아질 수 있는지는 예상할 수 없다.

따라서, eps만 설정할 경우 영원히 계산을 진행할 수도 있기 때문에, 학습횟수를 지정해 두어 학습횟수만큼만 iterate하게 해 주어야 한다.(종료조건으로 eps를 두어도 괜찮다)

## SGD와 minibatch

목적함수가 볼록(convex)하지 않을 경우, GD를 사용하면 초기값에 따라 local minima에 빠질 수 있다. - gradient descent의 특성상 극값인 local minima에서는 학습이 멈춘다.

SGD의 경우는 데이터의 일부로 gradient를 계산하여 업데이트하는 방식으로, minibatch 숫자만큼의 data만으로 한 step의 업데이트를 수행한다. 이 경우에는 n이 아니라 minibatch size b로 나누어주어야겠다.

gradient가 평균값이 아니라 여러 오차가 있는(전체 데이터의 gradient descent에 비해) 값이기 때문에, 확률적인 부분이 들어가 local minima에서 빠져나올 가능성이 생기게 된다. 즉, 볼록하지 않은 경우에도 최솟값을 찾을 수 있다는 것이다.

또한, GD의 경우에는 모든 dataset을 한 번에 계산하기 때문에 엄청난 크기의 matrix를 프로세서에 올려 놓아야 한다. 이는 GPU의 메모리를 초과하여 학습을 중단시키거나, cache locality를 훼손시킬 가능성이 있다.

CPU에서 GPU로 데이터를 순차적으로 보내주기 때문에, "CPU에서 데이터를 전부 GPU로 보내준다" -> "GPU에서 처리한다" 라는 순차적 관계를 조금 더 쪼개서 효율적으로 프로세서를 관리할 수도 있다.

랜덤성을 추가하기 위해 sample을 섞어서 minibatch 단위로 가져오는 데이터 제너레이터로 `DataLoader`를 사용하면 좋다.

## 학습률

학습률을 고정시키면, '나중에 들어온 값'의 영향력이 더 커질 수 있다. 

따라서, Robbints-Monro 조건(GLIE)를 만족시키는 학습률 스케쥴러를 쓰는 것이 좋다.

$$ \sum_t{\lambda_t} = \infty, \ \sum_t{\lambda_{t}^{2}} < \infty $$

이 두 식이 의미하는 바는, 러닝 레이트는 0으로 가서 한 번 학습할 때 과하게 움직이는 값이 0으로 수렴함에도, 계속 학습하면 얼마나 minima와의 거리가 멀든 minima에 도착할 수 있음을 의미한다.

이를 만족시키는 것은 data가 들어온 순서 t에 따라 $\frac{1}{t}$ 로 해 주는 것이다. 이렇게 하면, 나중에 들어온 값의 영향력과 원래 값의 영향력을 비슷하게 할 수 있고,

minima에 정확히 들어가지 않고, learning rate가 너무 커서 진동하는 것을 방지할 수 있다.

초기 데이터가 너무 큰 영향을 끼치지 않게 하기 위해,

$$ \eta = \frac{\eta_0}{\sqrt{t+c}} $$ 

로 초기 c값을 설정해 주기도 한다. (이 경우는 Robbints-Monro 조건을 만족하지는 않는다)

# Further: optimizer들

https://d2l.ai/chapter_optimization/momentum.html 를 참조하여 정리하였다.

기본적인 Gradient descent 알고리즘을 보완하기 위한 최적화 알고리즘들이다.



## Momentum

### IDEA: Leaky average

일반적으로 lr을 decreasing하는 방식으로 SGD를 쓰게 되면,

non-convex한 경우에 나중 data가 local minima를 빠져나가지 못하게 된다.

이를 방지하기 위해, 이전 값들의 gradient를 더해 주는 것을 Leaky average 방식이라고 한다.

이전 값들을 저장할 정도를 선택하는 hyperparameter $\beta$ 를 정한다(0과 1 사이)

update를 할 때 gradient 대신 사용할 $v_t = \beta v_{t+1}  + grad_t$ 를 구해 준다. 

이렇게 하면, 이전 값들이 (점점 줄어들지만) 영향을 계속 끼쳐 조금 Leaky하지만, 각각의 데이터의 gradient 영향력이 비슷해지게 된다.

이전 gradient 방향으로도 움직이는, 관성을 가지는 것으로 이해하면 좋다.

### 계산

Leaky average로 구한 $v_t$ 를 gradient 대신 사용한다. learning rate $\eta$ 는 그대로 사용.

## Adagrad

### IDEA : infrequent features

기본 아이디어는 "잘 나오지 않는(업데이트가 잘 안 되는 = Sparse) feature(는 나올 때 중요한 업데이트를 한다" 이다.

learning rate를 decreasing하게 되면, 희소한 feature의 경우에는 optimal value가 되기 전에 learning rate가 너무 작아져 버려 업데이트가 잘 되지 않을 수 있기에, 이를 보완하겠다는 것이다.


$$ \eta = \frac{\eta_0}{\sqrt{t+c}} $$

를 쓰는 대신에, 각각의 feature(i번째)에 대해

$$ \eta_i = \frac{\eta_0}{\sqrt{s(i,t)+c}} $$

를 사용해 주겠다는 것이다.

$s(i,t)$는 t번째까지의 gradient들의 제곱의 합이다.

사실 더해주는 $c = \epsilon$ 의 경우는 매우 작은 값으로, 분모가 0이 되지 않을 정도로만 설정해 주기 때문에,

학습율을 이전 같은 방향 gradient의 크기로 나누어 준다고 생각하면 된다.

즉, 

$$ \eta_i = \frac{\eta_0}{\sqrt{\sum{gradient_i}+c}} $$

이다. 물론, $\sum{gradient_i}$ 부분은 매번 계산하는 게 아니라 하나씩 더해주면 되기 때문에 계산이 크지는 않다.

## RMSProp

### IDEA

Adagrad의 경우, 결국 learning rate의 분모의 값이 커지며 non-convex의 손실함수에서 local minima에 빠지는 같은 문제에 빠지게 된다.

이를 위해, 이전 값과 현재 값을 비율 $\gamma$ 을 정해 더함으로써, 정규화를 진행한다.

$$ s_t \leftarrow \gamma s_{t-1} + (1 - \gamma)g_{t}^2 $$


정리하면

$$ s_t = (1-\gamma)(g_{t}^{2} + \gamma g_{t-1}^{2} ... ) $$

이 되어, 등비급수의 성질에 의해 $0 < \gamma < 1$ 에 대해 개략적으로 $(1-\gamma)\frac{1}{1-\gamma}(g)$ 가 되어, $ s_t $가 1로 정규화됨을 볼 수 있다.

### 계산

Adagrad에서 $s_t$ 만

$$ s_t = (1-\gamma)(g_{t}^{2} + \gamma g_{t-1}^{2} ... ) $$

로 바꾼다. 똑같이, 한 step에 한 번씩 update해 주므로, 모든 sum을 step마다 계
산할 필요는 없다.

## Adadelta

## Adam

Local minima에 빠지지 않기 위한 방법들이다.
