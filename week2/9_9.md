# 정리


## 아인슈타인 표기법

강의에 나온 함수들 뿐만 아니라, 좀더 많은 함수들도 알아보자.

차원이 높아짐에 따라.. `dot()`이나, `matmul()`함수가 어떻게 동작할지는 생각하기 어렵다.

텐서의 연산을 조금 더 직관적으로 표현하기 위해서는, `numpy.einsum`, `einops`를 사용하는 게 좋다.

`numpy.einsum`은 index의 변화를 string으로 받는다.

직관적으로, 우리가 사용하는 행렬곱은 'ik, kj -> ij'의 차원 변화를 갖는다.

이를 그대로 `numpy.einsum()`에 넣고, 두 행렬을 넣으면 `k`차원 벡터끼리의 내적을 계산해 행렬곱을 진행해 준다.

만약 'ij -> ji'식으로 하나의 행렬에 대한 식을 적으면, i와 j를 뒤집어 준다. 차원이 같아 하나의 값에 대한 sum을 하기 때문에 값이 유지되어, 행과 열만 바뀌는 것으로 보인다.

이떄, 'ii'등을 사용하면 행과 열이 같은 값들만 가져오는 식으로, trace 값들을 구할 수 있다.

'bii -> b'를 사용하면 b차원의 ii, 즉 대각성분만 더하는 식으로 작동한다.

`einops` 알아보기

`einops`는 `einsum`과 비슷하지만 조금 더 많은 함수들을 가지고 있다.

https://einops.rocks/ 에서 찾아보면..

`einops.rearrange`는 `view`나 `reshape`와 같지만, 차원을 알 필요가 없다.

`transpose`처럼, 차원의 순서만 알고 있으면 그것을 묶는 것이 가능하다.

묶는 것 뿐만 아니라 크기 8의 차원 b 을 4,2로 분리하려면, '(b db) -> b db', db = 2

이런 식으로 할 수 있다. 다만 이 경우에는 db로 b가 나누어진다는 것을 알아야 한다.

`repeat()`은 원하는 축에 대해 '(tile 축)', tile = 횟수 식으로,

`reduce()` 는 함수와 합칠 축을 찾아, 각각의 축에 함수를 적용시켜 텐서를 축소시킨다.

`reduce(A, '(x dx) (y dy) (z dz) -> x y z', 'min', dx=3, dy=4, dz=5)` 를 하면

A를 dx = 3, dy = 4, dz = 5인 block으로 나누어 min값을 넣은 tensor를 반환해 준다.

`np.einsum`외에도 `einops.einsum`이 있는 것 같다. 아마 같은 동작인듯 싶다.

과제 2번은 과제보다는 실습에 가까웠다.

주의할 점은, `einops`는 `np`에 있는 게 아니라 따로 `import`해야 하고,

`np.einsum()`과 다르게 각각의 변수를 띄워줘야 한다는 것이다.

`'b i j k'`이런 식으로.. 아마 붙어있는 것들을 하나로 취급하는 듯 하다.

## Gradient Descent

### 미분

일변수 함수 $y = f(x)$ 의 경우에는 단순히 x에 대한 미분값을 기울기로 해석할 수 있다.

이때, 기울기 부호의 방향으로 $x$가 움직이면 $f(x)$ 가 증가하고, 반대면 감소한다. 움직이는 크기는 $x \times f'(x)$ 로 볼 수 있지만, $x$ 가 커질수록 부정확해진다.

이를 다변수로 확장시키면,  **x** = $[x_1,x_2,x_3...,x_m]$ 에 대해 각각의 편미분값 $\partial_{x_i}$를 구하면

$\triangledown$ f(**x**) = $[\partial_{x_1},\partial_{x_2},\partial_{x_3},...,\partial_{x_m}]$ 로 구할 수 있다.

이 벡터에 **x**를 대입하면 하나의 방향 벡터가 나오는데, 이 벡터의 방향은 m차원 공간 상에서 목적함수가 가장 빠르게 증가하는 방향벡터를 의미한다. 크기는 얼마나 빠르게 증가하는지이다.

따라서, 여기에 -1을 곱해주면 가장 빠르게 감소하는 방향이다(하나의 m차원 hyperplane을 생각하면 그 반대 방향 벡터도 있음을 짐작할 수 있다)

### 알고리즘

기본적인 경사하강법은 학습률 learning rate lr, 종료조건 $\epsilon$ eps를 인자로 받는다.

learning rate는 학습이 진행되는 속도로, grad에 곱해지는 값이다. 

eps는 최대 오차로, 오차값이 이보다 작아지면 학습이 충분히 진행되었다고 생각하고 멈춘다.

의사코드를 적어보면

```
lr = 0.01, eps = 1e-5 #hyperparameter이므로, 알아서 설정

result = random #만약 공간이 convex하지 않다면, 이 초깃값에 따라 local minima에 빠질지가 결정된다.
while(grad.norm() < eps):
    grad = gradient(result)
    result -= lr * grad 
```

### 선형회귀에서의 적용

선형회귀에서의 목적식은

$$ \Vert \mathbf{y} - \mathbf{X} \beta \Vert_2 $$ 

이고ㅛ, 여기서 $\beta$ 를 찾는 것이 목적이다.

Gradient descent를 사용하기 전에, 이 목적식은 root를 취하므로 계산을 간단하게 하기 위해 제곱을 해서 사용한다.

norm이 음수가 아니기 때문에, 제곱한 값을 최소화하는 것과 원래 값을 최소화하는 것은 동치이다.

(RMSE를 최적화하는 대신, MSE를 최적화)

$$ \triangledown_{\beta} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2} = (\partial_{\beta_1}  \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2}, ... , \partial_{\beta_d} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2}) $$

를 구하면 된다. 

$y = f(x)$ 일 때, $y^2$ 를 x에 대해 미분하면 $2y'y$ 가 나온다. 위의 식에서 y는 $\mathbf{y} - \mathbf{X} \beta_i$ 가 된다. 왼쪽의 y는 x인 $\beta_i$ 와 무관하므로 오른쪽의 미분값만 계산하면

$\beta_i$가 곱해지는 부분은 $\mathbf{X}$ 의 $i$ 번째 열 뿐이므로, y'은 $-2\mathbf{X}_{i}^{T}$ 가 된다.

따라서 $(\partial_{\beta_i} =  -2\mathbf{X}_{i}^{T}(\mathbf(y)-\mathbf{X}\beta)$ 로 구할 수 있고, 이를 이어붙히면

$$ \triangledown_{\beta} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2} = -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) $$ 

가 된다.

하지만 이 값은 데이터가 많아질수록 커지는 특성을 가지고 있기 때문에, SSE 대신 MSE를 사용하기 위해 데이터의 갯수 n으로 나누어 준다.

$$ \frac{1}{n} \triangledown_{\beta} \Vert \mathbf{y} - \mathbf{X} \beta \Vert_{2}^{2} = -\frac{2}{n}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) $$ 

이 식을 gradient descent에 넣어서 활용한다.

$$ \beta^{(t+1)} = \beta{(t)} + \frac{2 * \lambda}{n}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) $$

이때 $2*\lambda$ 에서 2는 어차피 고정된 값이므로 이 자체를 learning rate로 설정한다.

gradient를 계산하는 부분만

`grad = - X.T @ (y - X @ beta)`

로 바꾸어 주면 된다.

# Futher: optimizer들
