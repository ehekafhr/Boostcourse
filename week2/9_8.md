# 강의정리

강의 내용뿐만 아니라, 조금 더 디테일한 수식도 다루어 보았다.

## 벡터

숫자의 1차원 배열로, 배열의 원소 개수를 "차원(dimension)"이라고 한다. numpy에서는 1차원 array로, 1중 괄호의 `numpy.array()`로 구현한다.

*Tensor나 matrix의 x번째 차원과는 다른 의미로, 크기 개념으로 생각해야 한다.

크기 d차원 위 공간의 한 점으로 이해할 수 있다.

### 벡터의 정의

벡터의 정의라기보다는 벡터가 만족해야 할 '조건'이다.

$$ (1) \ \Vert{\lambda x}\Vert = |\lambda|\Vert{x}\Vert $$

$$ (2) \ \Vert{x+y}\Vert \leq  \Vert{x}\Vert +\Vert{y}\Vert $$

$$ (3) \ \Vert{x}\Vert \geq 0 \ and \  \Vert{x}\Vert = 0 <=> x = 0 $$

첫번째는 스칼라곱은 노름의 크기에 그대로 곱해진다는 조건,

두번째는 두 벡터의 합의 노름은 각각의 벡터의 노름의 합보다 작아야 한다는 조건.

셋째는 노름은 음수일 수 없고, 노름이 0이라는 것은 x의 모든 값이 0 이라는 조건이다.

일반적으로 노름은 L2 norm(유클리드 norm-Ridge 회귀)을 사용하지만, L1 norm(맨해튼 거리-Lasso 회귀)를 사용할 때도 있다. L2 norm을 사용하면 우리가 일반적으로 사용하는 유클리드 벡터가 된다.

### 벡터의 계산

Field 위의 벡터 공간에서, 두 가지의 연산(+,$\cdot$)을 갖는 경우 다음의 조건을 갖는다.

+에 대한 조건: 결합 법칙, 교환 법칙, 항등원 0, 역원 존재

$\cdot$에 대한 조건: 스칼라곱 유지, 항등원 1, 분배법칙

위의 조건에서 벡터의 상수곱(각 원소에 상수배), 벡터의 +(원소끼리의 합), $\cdot$(내적, 원소끼리의 곱 = 성분곱 $\odot$ 의 합)을 찾을 수 있다. -는 +의 역이다.



하지만 두 벡터 사이의 거리를 나타내는 노름은 원하는 대로 정할 수 있다. 두 벡터를 뺀 값의 크기로 계산되는데, 크기를 재는 방법은 하나로 정해지지 않는다. 

노름의 특성상 어떤 쪽에서 빼든 값은 같기 때문에 거리는 하나로 존재한다.

만약 L2 norm을 사용한다면, 내적과 노름을 통한 각도 계산이 가능하다.

내적은 벡터의 각각의 요소를 요소별로 곱한 후 더한 값으로, 어떤 벡터를 다른 벡터에 project한 후 스칼라곱을 하는 것이다. 

이를 식으로 나타내면(a를 b에 project한다고 하자, 거꾸로 해도 값은 같다) a벡터와 b벡터의 각도 $\theta$에 대해

$$ proj(a) = \Vert b \Vert cos \theta $$

이를 통해 내적을 계산하면,

$$ \vec{a}\cdot \vec{b} =\Vert a \Vert    (\Vert b \Vert  cos\theta )$$

이를 통해 코사인값을 구하면

$$ cos\theta == \frac{\vec{a}\cdot \vec{b}}{\Vert a \Vert \Vert b \Vert} $$

이 된다. 여기서 코사인 역함수만 취하면 값이 나온다.

## 행렬

### 정의와 이해

행렬은 벡터를 원소로 갖는 2차원 배열이다. (벡터처럼 원소의 개수가 2개라는 뜻이 아니라, 축이 2개)

행과 열의 index를 갖는다. 이때, 일반적으로 행 번호를 i, 열 번호를 j라고 하고, 행의 갯수를 n, 열의 갯수를 m이라고 한다. 

행렬을 행 방향으로 보면 m개의 행 벡터가, 열 방향으로 보면 n개의 열 벡터를 볼 수 있다. numpy에서는 행 벡터를 기본으로 본다.

위에서 벡터를 d차원(벡터의 차원) 공간의 한 점이라고 했는데, 그렇다면 행렬은 그러한 벡터의 집합이라고 볼 수 있다. 

행렬의 한 값 $x_{ij}$는 i번째 행, 즉 i번째 벡터의 j번째 요소를 의미한다. i번째 벡터의 d차원 위에서의 j차원 방향의 값이다.

또다른 행렬을 보는 관점은 벡터의 선형 변환을 일으키는 연산자로 보는 것이다.

($A$행렬의 크기 n*m, $x$의 차원 m, $z$의 차원 n)
$Ax = z$ 에서 $A$ 행렬은 $x$를 m차원에서 n차원으로 선형 변환시켜 보내는 역할을 한다.

### 연산

행렬의 행과 열을 바꾼 행렬을 전치행렬, 수식으로는 행렬 $A$의 전치행렬을 $A^T$로 표현한다.

행렬의 크기가 같다면, 같은 차원의 벡터를 같은 개수만큼 가지고 있다는 의미이다. 이러한 경우, j번째 벡터끼리의 기본 연산을 수행할 수 있다. (덧셈, 뺄셈, 성분곱)

행렬곱 연산은 왼쪽 Matrix의 i번째 행벡터와, 오른쪽 Matrix의 j번째 열벡터 사이의 내적으로 i$\times$j Matrix가 나오게 된다.

이때, 각각의 벡터의 크기가 같아야 하기 때문에 왼쪽 행렬 A의 열의 개수와 오른쪽 행렬 B의 행의 개수가 같아야 한다.

A가 m$\times$n 행렬, B가 n$\times$k 행렬이라고 하면

$$ (AB)_{ij} = \sum_{k=1}^{n}{a_{ik}b_{kj}} $$ 

로 나타낼 수 있다. @를 통해 numpy에서 행렬곱을 할 수 있다. 행렬곱은 차원에서부터 알 수 있듯이, 교환법칙이 성립하지 않는다.

`np.inner()`는 i번째 행벡터와 j번째 행벡터의 내적을 i$\times$j행렬로 표현한다. 이는 단순히 $AB^T$와 같다.

행렬 $A$를 선형 변환으로 본다면, 이를 되돌리는 역행렬 $A^{-1}$을 생각해 볼 수 있다.

어떠한 벡터, 행렬도 그대로 유지하는(차원 크기는 맞아야 한다) 항등행렬 $I$는 $I_{ij}$가 $i=j$인 원소만 1인 행렬이다.

역행렬은, 어떠한 행렬과 곱해서 항등행렬이 만들어지는 행렬을 의미한다.

이때, 역행렬의 역행렬은 원래 행렬이 된다.

$$AA^{-1} = A^{-1}A = I$$

차원의 크기를 생각해보면 당연히 역행렬은 행과 열의 크기가 같을 때만 구할 수 있다.

`np.linalg.inv()`를 통해 numpy에서 역행렬을 구할 수 있다.

### 연립방정식

이를 통해 연립방정식을 풀 수 있는데, 연립방정식의 계수 행렬을 A라고 하고 상수 행렬을 b, 미지수들을 x라 하면

$$ Ax = b $$ 
꼴로 연립방정식을 나타낼 수 있다. 양변 왼쪽에 $A$의 역행렬 $A^{-1}$을 곱해주면

$$ A^{-1}Ax = A^{-1}b$$

$$ Ix = A^{-1}b $$

$$ x = A^{-1}b$$

이다.

#### 역행렬의 조건

행렬식(determinant)는 행렬을 구성하는 벡터들을 변으로 하는 polytope의 부피의 크기와 같다. 행의 크기와 열의 크기가 같아야만 구할 수 있다.

예를 들어, 2차원 행렬의 행렬식은 두 벡터를 변으로 하는 평행사변형의 넓이가 된다.

그렇다면 행렬식이 0이라는 것은, n*n 행렬에서 n차원보다 적은 차원만으로 벡터들을 표현할 수 있다는 의미가 된다.

그렇지 않다면, 행렬의 벡터들을 표현하려면 n차원이 필요하고 벡터들이 나타내는 다포체의 부피가 0이 아님을 의미한다.

이와 동치인 표현으로는 "행렬 A의 rank(A) = n"이 있다.

rank(A)는 A를 구성하는 벡터 중, 선형독립인 벡터의 개수를 의미한다.

선형 독립이라는 것은, 어떠한 벡터가 다른 벡터들의 선형 결합(각각의 벡터에 상수배를 곱해서 더합)으로 나타내어지지 않음을 의미한다.

$v_j \neq \sum_{i}{c_iv_i} \ \ \forall c_i \in \R, i\neq j $일 때 $v_j$를 선형독립이라고 한다.

따라서, 행렬식이 0이라는 것은 행렬이 나타낼 수 있는 차원이 하나 이상 모자라다는 뜻이고, 따라서 해가 부정이거나 불능 상태에 빠지게 된다.
불능 상태는 벡터의 선형 결합으로 b를 만들 수 없는 상태이다. 3차원 공간에서 선형 결합으로 면 위의 점들만 만들 수 있다면, b가 그 면 위에 없다면 불능 상태가 된다.

반대로 b가 그 면 위에 있다면, 위의 식에서 선형 독립이 아닌 $v_j$가 하나 존재하므로
 $v_j - \sum_{i}{c_iv_i} = 0 \ \  i\neq j $ 를 만족시키는 $c_i$들에 대해

 $x_0$ = [1, $c_1$, ... , $c_{n-1}$]이라 하면
 $Ax_0 = 0$인 영공간이 된다. 여기에 상수배를 곱해 원래의 해에 더해서, 무한히 많은 해를 찾을 수 있다. 이 상태를 부정이라고 한다.

 ### 유사역행렬

 만약 n과 m이 다르다면, $Ax = b$를 어떻게 풀어야 할까?

 유사역행렬(numpy에서는 `numpy.linalg.pinv()`)을 통해서 계산할 수 있다.

 n * m 행렬 A에 대해,

 $ n \geq m $인 경우에는 유사역행렬 $A^+ = (A^TA)^{-1}A^T$
 $ m \geq n $인 경우에는 유사역행렬 $A^+ = A^T(AA^T)^{-1}$

 이유는 차원에 있다. 행렬 A의 rank는 당연히 min(n,m)를 넘을 수 없다. 그렇기 때문에, 큰 쪽의 행렬을 만들 경우에는 역행렬을 계산할 수 없다.
 또한, rank값이 min(n,m)이 아닌 경우 또한 역행렬을 계산할 수 없기에 유사역행렬을 만들 수 없다.

 유사역행렬은 역행렬처럼, $A^+A = I$를 만족한다.
$$A^+A = (A^TA)^{-1}(A^TA) = I$$


 이를 통해, $Ax = b$의 양쪽에 유사역행렬을 곱해 주면

 $A^+Ax = A^+b$
 $Ix = A^+b$
 $x = A^+b$가 된다.

 유사역행렬을 통한 계산은 불능인 경우에도 가능하다.

 데이터가 변수 개수보다 많은 경우, 일반적으로 완벽한 해를 구할 수는 없고 오차를 최소화하는 값을 구하게 된다.

 선형회귀 모델 $$X\beta = \hat{y} \approx y$$

 에서 $\beta$는 

 $min_{\beta}\Vert y-\hat{y} \Vert $ where $ \beta = X^+y$
 
 로 계산한다.

단순히 유사역행렬을 사용하면 입력으로 넣은 변수들의 선형 선형 결합만 가능하다. 선형 회귀를 위해서는 상수항 Intercept 또한 필요하기 때문에, 모든 행에 상수항 Intercept 1을 넣어주는 작업이 필요하다.

## 행렬의 분해

### 고유값 분해(eigendecomposition)

#### 고유벡터 eigenvector, 고유값 eigenvalue

어떤 $n\times n$ 행렬 $A$에 대해 벡터 $v$를 곱한 값이,

$Av = \lambda v$를 만족하는 경우 $v$를 고유벡터, $\lambda$를 고유값이라고 한다.

행렬곱을 취했을 때 크기만 변한다.

이러한 $v, \lambda$쌍은 n개 존재할 수 있다.

eigenvector가 n개 존재한다고 했을 때 k= 1,2,...,n에 대해

$$ Av_k = \lambda_k v_k$$

$k$번째 값을 열벡터로 이어붙히면

$V = [v_1, v_2, v_3 ... , v_n]$ 라 하면,

$A[v_1,v_2,...,v_n] = [\lambda_1v_1,\lambda_2v_2,...,\lambda_nv_n] = V\begin{pmatrix}
\lambda_1 & 0 & ... & 0 \\
0 & \lambda_2 & 0 & 0 \\
... & 0 & ... & 0 \\
0 & 0 & 0 & \lambda_n \\
\end{pmatrix}$

고유값을 대각성분으로 갖는 행렬을  $\Lambda$라 하면

$$ AV = V\Lambda $$를 만족한다.

양쪽에 $V^{-1}$을 곱해주면 A를

$$ A = V\Lambda V^{-1} $$
로 분해할 수 있다. 이를 고유값분해라고 한다.

### PCA(주성분 분석)

저차원공간으로 데이터를 정사영시킬 때, 오차를 최소화하는 저차원 공간을 찾는 방법이다.

정보손실을 최소화하는 방식으로 찾는다. 즉, 데이터와 저차원 공간 사이의 거리의 합을 최소화하는 기저벡터 $v, \Vert v \Vert = 1$ 들을 찾는다.

$$ v^T XX^T v $$를 최대화하는 문제가 된다. 즉, 정사영된 데이터의 분산을 가장 크게 만드는 v를 찾는 문제가 된다.

라그랑지 승수법을 통해 계산한

$XX^Tv = \lambda v  $ 을 대입하면

$$ \lambda \Vert v \Vert ^2 = \lambda $$

를 최대화하는 문제가 된다.

따라서 고유값들 중 가장 큰 값을 순서대로 가져오고, 그에 해당하는 고유값을 가져오면 된다.

$$ XX^T = V\Lambda V^{-1} $$ 를 통해 고유값을 구한다.


### 특이값 분해(singular value decomposition)

고유값 분해는 정사각 행렬에만 사용가능하다.

특이값 분해는 그럴 필요 없이, 모든 행렬에 사용 가능한 분해이다.

$$ A = U\Sigma V^T $$
$A$는 $n \times m $, $U, V^T$는 $n \times n, m \times m$의 orthogonal 행렬, $\Sigma$는 $n \times m$ 대각 행렬이다.

$$ A = \sum_{k=1}^{rank(A)}{\sigma_k u_k v_{k}^{T}} $$

에서, $\sigma_k$를 singular value라고 한다.

이를 통해 rank가 min(n,m)이 아닌 행렬의 역행렬을 계산 가능하다.

$$ A = U\Sigma V^T \Longrightarrow V\Sigma^+U^T$$

에서, $\Sigma^+$는 특이값들의 역수를 취한 행렬이다.

역행렬 계산보다 SVD가 빠르기 때문에, 유사역행렬은 SVD로 사용한다.
`numpy.linalg.pinv` 또한 SVD로 유사역행렬을 계산한다.


# 피어세션

## 알고리즘: DFS와 BFS
DFS와 BFS

재귀함수로 구현한 DFS는 이해하기 쉽지만, python의 경우는 최적화가 좋지 않고, `sys`를 허용하지 않을 경우 깊이 제한이 걸린다.

따라서, stack형태로 바꾸어 주는 것이 가능하다면 좋다.

BFS는 특정 경우에 사용하면 좋지만, 탐색 범위가 넓어지면 메모리 문제가 발생 가능하다.

## Questions

Determinant는 부피를 의미한다..

Rank와 Determinant, 역행렬의 관계에 대한 이야기가 있었다.

rank가 n이라는 것은 vector들을 선형 결합해서 n차원의 모든 값을 만들 수 있다는 뜻이다.

Determinant가 0이 아니라는 것은 부피가 존재한다는 것이고, 부피가 0이라는 것은 벡터들을 n-1차원에 모두 넣을 수 있다는 것을 의미한다.

다시 행렬을 선형 변환으로 보면, Determinant가 0 = rank가 n-1이라는 것은 어떠한 n-1차원 공간에 벡터를 보낸다는 것이다. 이렇게 된 벡터를 다시 변환해도, 이 벡터를 포함하는 n-1차원 공간은 반드시 존재한다.

예를 들어, (0,0,1)과 (0,1,0) 두 벡터의 합으로 포함되는 벡터를 아무리 더해도 x축의 값이 나오게 하는 행렬은 존재할 수 없다.

# 과제

역행렬을 구할 때는 행렬식을 정말 많이 계산했다..

adj matrix를 구하기 위해 `np.delete()`를 통해 행과 열이 하나씩 줄어든 행렬의 행렬식을 행렬 크기만큼 구했으니..

별로 효율적인 방법은 아니었던 것 같다.

고윳값과 고유벡터는 `np.linalg.eigh()`를 쓰지 않고 도저히 어떻게 구할지 감이 안 잡혔다.

결국 가장 간단한 방법인 power method를 사용했다.

고윳값이 n개라는 가정을 해야 쓸 수 있는 방법인데.. 일단 구현 가능한 것부터 해 보기로 한다.

초깃값 $x_0$을 아무렇게나 잡으면..(첫번째 값을 0으로 하지는 말자) 

모든 고윳값 $v_i$의 선형 결합으로 $x_0$을 나타낼 수 있다(고윳값이 n개이므로)

그렇다면 행렬 $A$를 양쪽에 곱할 때마다, 고유벡터의 성질에 의해 단순히 고유벡터에 고윳값이 곱해진다.

이를 충분히 반복하면, 고윳값이 가장 큰 고유벡터만 남고, 이때 A를 곱했을 때 증가하는 비율을 재면 고윳값을 구할 수 있다.

또한 고윳값 분해를 하면 matrix $A$를 다음과 같이 나타낼 수 있다.

$$ A = \sum_{i=1}^{n}{\lambda_i v_i v_{i}^{T}} $$

앞에서 구한 가장 큰 고윳값과 그 고유벡터가 나타내는 부분 $\lambda_i v_i v_{i}^{T}$를 빼주고 n번 반복하면 고윳값 n개를 구할 수 있다.

전치행렬과 행렬곱은 단순한 for문중첩이라 어렵지 않았다.


# TODO

라그랑지 승수법과 이를 통한 PCA 증명과정 보기

SVD 분해 과정 보기.
