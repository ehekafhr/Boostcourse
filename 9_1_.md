# 9/1 학습 정리

### PyTorch
Hugging face, tesla, uber 등의 기술 개발에 사용되고 있으며, 2020년경을 전후로 TensorFlow를 제치고 가장 많이 사용하는 프레임워크가 되고 있다.

#### Q1. 어떻게 PyTorch가 TensorfFlow를 제칠 수 있었을까?
강의에 나온 동적 계산 그래프는 텐서플로우 기준으로는 TensorFlow 2.0(2019)에서부터 지원이 된 것으로 보인다. 실제로 17-19년도 사이에 이동이 가장 잦았던 것으로 보아, 이때 대이동이 일어나고 라이브러리 등을 확보해서 점유율을 굳힌 것 같다.

### Tensor
Pytorch의 데이터 구조로, Numpy의 다차원 배열에 해당.
0-D: Scalar, 1-D: Vector, 2-D: Matrix를 표현한다.
확장하여, N-D Tensor는 (N-1)-D Tensor들이 쌓여 있는 것이다.


### 데이터 타입
정수형: 소수 부분이 없는 데이터로, 8, 16(short), 32(기본 int), 64bit(long)가 있으며 8bit(1 byte)의 경우에는 부호 없는 데이터 형식이 있다. torch.intX형식으로 표현하며, 8bit unsigned의 경우는 unit8로 한다.

추가: Signed int의 경우에는 음수를 2’s complement로 표현한다. 절댓값에 해당하는 양수 bit들을 전부 뒤집고, +1을 한다. 계산의 편의성이 좋아지고 표현할 수 있는 범위가 1 늘어나는 효과가 있다.

부동소수점(float): 고정 소수점의 경우에는 너무 많은 메모리를 요구하는 문제가 있기 때문에, 가수부와 지수부를 분리한 부동소수점을 사용한다. 끝의 1bit가 부호, 8bit의 지수부와 23bit의 가수부로 이루어져 있으며, 가수부에 지수부만큼 2의 exponent를 곱해준다. Double의 경우는 11bit의 지수부, 52bit의 가수부로 이루어진다

#### Q2. 부동소수점이 고정소수점보다 얼마나 좋은가?
부동소수점은 지수부 8bit, 가수부 23bit로, 표현 범위가 1.175494351E-38에서 3.402823466E+38까지로, 아주 작은 수부터 아주 큰 수까지를 표현할 수 있다. 고정소수점으로 이 범위를 표현하려면, 정수부와 소수부가 메모리를 각각 100bit 넘게 차지하게 된다. 
200bit라고 생각해도, 메모리로 6배가 넘는 차이이다.
### 타입 캐스팅
생성된 Tensor에 .(타입 이름)()을 통해 타입을 변환할 수 있다.

+표현 범위가 넓은 Type에서 좁은 Type으로 변환하면, 소수점 부분이나 중요하지 않은 부분이 잘리거나, 음수를 uint로 변환할 경우 완전히 값이 뒤집힐 수 있으니 조심해서 캐스팅해야 한다.
### Tensor 기본 함수, 메서드
min, max, sum, prod, mean, var, std 함수 전부 Tensor의 모든 요소들에 대한 함숫값을 계산한다.

+인자로 차원을 넣어 주면, 해당하는 차원에 대한 계산을 수행한 Tensor를 대신 반환한다.

Tensor의 특징을 확인하기 위해서는, dim() – 차원의 수, size() or shape – 모양, numel() – element의 총 개수 등을 사용할 수 있다.

### Tensor의 생성
0이나 1로 초기화된 Tensor:  `torch.zeros/ones(dim)`으로 dim 차원의 Tensor 생성.

연속균등분포 (0,1 사이) Tensor: `torch.rand(dim)`

표준정규분포 Tensor: `torch.randn(dim)`

균등 간격 Tensor: `torch.arange(start,end,step)` – python의 range처럼, end값은 포함하지 않는다.

초기화되지 않은 Tensor: `torch.empty(dim)`: 값을 바로 바꿀 경우, 불필요하게 초기화하는 계산 비용을 넣을 필요가 없다.

Tensor에 채우기: `(tensor).fill_(값)`으로 채울 수 있다.

다른 특정한 Tensor와 Shape가 같은 Tensor를 생성하고 싶으면, `torch.(초기화값)_like(원하는 Shape의 Tensor)`를 통해 같은 모양의 Tensor를 생성 가능.

Python List의 경우 `torch.tensor(list)`, numpy의 경우 t`orch.from_Numpy(nparray)`로 Tensor로 변환할 수 있다.

CPU Tensor는 `torch.(type)Tensor(Tensor값)`으로 생성 가능

Tensor의 복제는 `clone()`와 `detach()`가 있는데, `detach()`의 차이는 추후 나올 예정이다.

GPU: GPU는 병렬 처리 능력이 뛰어나, 시간 단축과 에너지 절약을 할 수 있다.

먼저 Tensor의 device를 `.device`로 확인하고, `torch.cuda.is_available()`로 CUDA가 사용 가능한지 확인 후 `.to(‘cuda’) 혹은 .cuda()`로 GPU로 Tensor를 보낼 수 있다.

반대로, `.to(‘cpu’)` 혹은 `.cpu()`로 CPU로 Tensor를 보낼 수도 있다.

### Indexing
프로그래밍 언어의 다중 array의 접근과 비슷하지만,

[][]로 괄호를 여러 번 치는 대신 [0-D,1-D,2-D]식으로 표현한다. 

Python indexing과 마찬가지로 음수도 가능하며, slicing도 가능하다.

: 대신 …으로 차원의 모든 범위를 설정할 수도 있다.

이때 만들어진 Sub Tensor는 메모리상에 contiguous하지 않을 수 있다. 이는 is_contiguous()로 확인 가능하고, contiguous하지 않을 경우 .contiguous()로 contiguous하게 만들 수 있다.

### 모양변경
만약 Tensor가 contiguous하다면, view()메서드를 통해 모양을 바꿀 수 있다.

이때, 마지막 차원에는 그냥 -1을 써도 자동으로 남은 차원을 할당해 준다. 

Reshape()는 contiguous하지 않은 경우에도 사용 가능하지만, view()에 비해 자원 사용량이 많기에 contiguous한 Tensor에는 view()를 써야 한다.

Q3. View와 Reshape, Flatten에서 모양이 바뀔 때 값들의 위치는 어떻게 되는가?

메모리에 저장된 순서를 index로 그대로 유지한 채 값을 배치한다.

이때, View의 경우에는 shape만 바꾸고 메모리를 건드리지 않지만, reshape는 새로운 텐서를 만들기 때문에 자원 사용량이 차이가 난다.

`flatten()`은 인자에 따라, `flatten(텐서)`는 1차원으로, `flatten(텐서,x)`는 x차원부터, `flatten(텐서,x,y)`의 경우에는 x~y차원까지를 평탄화한다.

`.transpose()`의 경우에는 축을 바꿀 수 있다. 두 차원 인덱스를 받아서, 축을 바꾼다.
```
q= [[0,1,2],[3,4,5]]
p = q.transpose(0,1)
```
일때,
p = [ [0,3],[1,4],[2,5] ]가 된다.

`.squeeze()`함수는 dim이 1인 차원들을 삭제하는 함수이다. 차원 인덱스를 입력하면, 해당하는 index의 차원만 squeeze한다. 반대로 .`unsqueeze(dim)`의 경우에는, dim 위치에 1짜리 차원을 생성하고 원래 있던 차원과 그 이후의 차원을 한 칸 올린다.

`stack()` 함수는 Tensor들을 dim-0짜리 축을 생성하여 결합하는 코드 표현이다. dim을 입력하여 결합할 축을 결정할 수도 있다.
