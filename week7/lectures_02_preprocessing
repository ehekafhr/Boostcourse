# 전처리

## 텍스트 데이터 처리의 어려움

텍스트 데이터는 비정형 데이터로 분류된다.

특히, 이미지처럼 그리드 등의 구조가 없기 때문에 기계가 바로 이해하기 어려운 구조이다. 또한, 하나의 문장, 하나의 문단이라도 길이가 다르고 하나의 단어의 길이도 다르다.

아무 의미가 없어 보이는 HTML 태그, 반복되는 초성(한굴의 특징)등의 노이즈가 발생하기도 한다.

텍스트 데이터는 "문맥 의존성" 과 "표현의 모호성"이라는 두 가지 문제가 있다.

첫번째는 같은 단어라도 동음이의어인 경우에도 하나로 Tokenize할 수도 있다는 문제이고, 두 번째는 "같은 단어"의 변형을 전혀 다른 단어로 학습해버리는 문제이다.

또한 문장 전체의 구조에 영향을 받는 구조적 의존성이 있다.

이러한 문제를 처리하지 않고 텍스트 데이터를 하게 되면, GIGO(Garbage In, Garbage Out)처럼 쓰레기가 나올 숳 있다.

따라서, 원본 텍스트를 정제 - 토큰화 - 표준화 - 벡터화 하는 과정에서 정제와 표준화의 역할이 중요하다.

## 전처리의 효과

학습 효율을 증대시켜 성능을 향상시키고, 일반화 능력 향상과 인사이트 발굴 효과를 누릴 수 있다.

하지만, 현재의 LLM 등의 거대한 모델의 경우에는 이러한 노이즈 또한 하나의 정보로 받아들일 수 있기 때문에 정제는 최소화하는 추세이다.

## 텍스트 정제

소문자 변환: 대문자와 소문자는 큰 의미가 없기에, 같은 단어로 만들어 준다.

구두점 제거: 문장 부호를 제거해 준다.

정규 표현식을 통한 정제: HTML, URL 등을 제거하거나, [HTML]등의 하나의 토큰으로 만들어 준다.

분석 목적에 따라 텍스트 정제의 방식은 다를 수도 있다. 감정 분석의 경우에는, "ㅋ"의 숫자에 따라 다른 감정으로 인식해야 할 수도 있다.

하지만, 정보 추출의 경우에는 그러한 정보는 의미가 없고, 반대로 숫자 데이터가 큰 의미를 가질 수 있다.

"중요한 정보"를 남기고 "중요하지 않은"정보를 제거하는 규칙을 세워야 한다.

## 텍스트 토큰화

토큰의 단위는 단어, 형태소, 서브워드 단위가 있다

단어는 띄어쓰기 단위, 형태소는 "의미를 갖는 가장 작은 단위", 서브워드는 더 분해를 진행해 신조어, OOV 문제를 해결하기 위해 나온 방식으로, 현재 가장 많이 사용되는 방식이다.

특히 한국어의 경우에는 띄어쓰기 단위를 사용할 경우 조사가 붙는 특성이 있어 제대로 학습되지 않을 가능성이 높다.

그렇기 때문에, KoNLPy 등에서는 조사를 분리하는 subword 방식을 사용하게 된다.

## 텍스트 표준화

단어의 표현을 통일하는 것으로, 불용어를 제거하고 어휘를 표준화한다.

Apple와 apple은 같은 토큰이라는 것이고, 조사나 관사 등이 의미가 없다고 판단되면 이 또한 제거하고 단어를 원형으로 바꿀 수 있다.

## 텍스트 벡터화

표준화된 텍스트들을 사전에서 어디에 위치하는지 1로 표시한 one-hot vector 혹은 문장 자체에서 count하는 Bag-of-Words 방식을 사용할 수 있다. 다만, BoW 방식은 단어의 순서를 무시해 문맥이 사라질 수 있기 때문에 조심해야 한다.

단순히 one-hot encoding 을 하는 것보다는, 그 이후 Word2Vec이나 Glove 등 주변의 단어를 통해 학습시킨 벡터를 사용하거나, 문맥까지 고려한 BERT 등의 임베딩 방식을 사용하는 것이 좋다.
