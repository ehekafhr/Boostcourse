# 9/2 학습 정리
### Tensor의 모양변경

`stack()`함수는 텐서들을 "새로운 차원"으로 묶는다.

`cat()`함수는 기존의 차원을 유지하며 Tensor들을 연결한다.(기본 dim = 0)

이때, 묶는 차원 외의 다른 차원의 크기가 일치해야 `cat()`함수가 오류를 뱉지 않기에, `view()`와 `reshape()`로 차원을 미리 맞출 필요가 있다.

`expand()`함수는 주어진 차원의 크기가 1일 때, 해당 차원의 크기를 확장한다.

이에 반해 `repeat()`함수는 제한 없이, 각각의 차원을 몇 번 반복할지를 인수로 받아 확장한다.

`expand()`함수는 추가 메모리 할당 없이 복사하지만, `repeat()`함수는 추가 메모리를 할당하기 때문에 메모리 관점에서 좋지 않음.

### Tensor의 기초 연산

#### 산술연산

Tensor에 대해, `add()`, `sub()`, `mul()`, `div()`, `pow()`는 elementwise하게 산술 연산을 진행한다.

in-place로 추가 메모리 없이, `.add_()`, `.sub_()`, `.mul_()`, `.div_()`, `.pow_()`를 사용할 수 있다.

Tensor 두 개의 크기가 다른 경우, 다른 하나의 크기를 키워서 연산을 진행한다.

#### 비교연산

두 Tensor를 비교하면, 같은 크기의 Boolean Tensor가 나온다.

`eq()`(동일한지), `ne()`(다른지), `gt()`(초과), `ge()`(이상), `lt()`(미만), `le()`(이하)가 있다.

#### 논리연산

AND 연산 $\bigwedge$ `torch.logical_and()`

OR 연산  $\bigvee$ `torch.logical_or()`

XOR 연산 $\oplus$ `torch.logical_xor()`

| X     | Y     |X $\bigwedge$ Y|X $\bigvee$ Y|X $\oplus$ Y |
| :-----| :---: | ----: | -----: | -----: |
| T     | T     | T     |  T     |  F     | 
| T     | F     | F     |  T     |  T     | 
| F     | T     | F     |  T     |  T     | 
| F     | F     | F     |  F     |  F     | 



#### 크기가 다른 연산에서, 어떻게 다른 하나의 텐서를 확장하는가? 또, 가능한 조건은 무엇인가?

-> `expand()`와 같은 방식으로 동작한다.
`expand()`와 같이 확장하는 dim이 1이거나, 아예 존재하지 않아서 `unsqueeze()`후에 확장을 하는 경우가 아니면 오류가 발생한다.


### Tensor의 노름

Ln노름의 정의(n을 p로도 받는다)

$$||x||_p = \sqrt[p]{\sum_{i=1}^{n}{|x_i|^p}} $$

p = 1일 때 맨해튼 노름으로 절댓값들의 합,

p = 2일 때 유클리디안 노름으로 피타고라스 정리를 통한 실제 거리이다.

`torch.norm(tensor,p)`

$L\infty$ norm

n이 커짐에 따라, Ln노름은 "가장 큰 절댓값"으로 수렴한다.

### 유사도

두 1-D Tensor(Vector)가 얼마나 유사한지에 대한 정도.

군집화 Algorithm 등에 사용.

#### 맨해튼 유사도

1-D Tensor 사이의 거리를 맨해튼 거리의 역수로 변환

$$ Manhattan Similarity = \frac{1}{1+\sum_{i=1}^{n}{|x_i-y_i}} $$

이때, 분모에 1을 더해주는 이유는 맨해튼 거리가 0일 경우 정의가 불가능함을 보완하기 위함이다.

#### 유클리드 유사도

1-D Tensor 사이의 거리를 유클리드 거리의 역수로 변환

$$ Euclidean Similarity = \frac{1}{1+\sqrt{\sum_{i=1}^{2}{|x_i|^2}}} $$

똑같이 거리가 0인 경우를 위해 분모에 1을 더해 준다.

#### 코사인 유사도

두 1-D Tensor 사이의 각도를 측정

내적(`torch.dot()`)과 두 벡터의 절댓값(`torch.norm(p=2)`)을 통해 코사인값을 측정한다.

$$ x\cdot y = ||x||_2cos\theta \times ||y||_2 = ||x||_2||y||_2cos\theta $$

정리하면,

$$ cos(x,y) = \frac{x\cdot y}{||x||_2||y||_2} $$

를 유사도로 사용한다.

### 2D 행렬 곱셈

신경망 구현의 핵심으로, 두 행렬을 결합하여 새로운 행렬을 생성

이때, 앞 Tensor의 열의 수와 뒤 Tensor의 행의 수가 일치해야 한다.

D,E 행렬에 대해

$$ D \times E = \sum_l{f_{il} \cdot g_{il}} $$

로, D의 행과 E의 열이 유지된다.

`D.matmul(E)`

`D.mm(E)`

`D @ E`로 사용 가능하다.

이를 활용하여, 대칭 이동 등을 수행 가능하다.
$$
\begin{equation}
   \begin{bmatrix} 
   0 & 0 & 1  \\
   0 & 1 & 0  \\
   1 & 0 & 0  \\
   \end{bmatrix} 
\end{equation}
$$
이러한 행렬은 y축을 기준으로 이미지를 뒤집는다(좌우 대칭)

#### 이러한 변환을 하는 Matrix가 추가로 있는가?

상하 변환 matrix

-> 좌우 변환 matrix를 뒤가 아니라 앞에 곱하면 된다.


### 선형회귀

#### 의미
 주어진 training data로, 특징 변수와 목표 변수 사이의 선형 관계를 분석하고 이를 바탕으로 모델을 학습시켜, training data가 아닌 새로운 데이터의 결과를 연속적 숫자 값으로 예측

#### 상관관계

두 변수 간의 선형 관계를 파악

양의 관계인지 음의 관계인지, 얼마나 관계가 있는지 파악 후 높은 상관관계를 가지는 변수들 파악

상관관계 수식(피어슨 상관계수, 표본상관계수)

$$ r_{xt} = \frac{\sum_{i=1}^{n}{(x_i-\bar{x})(t_i-\bar{t})}}{\sqrt{\sum_{i=1}^{n}{(x_i-\bar{x})^2 \sum_{i=1}^{n}{(t_i-\bar{t})^2}}}}$$

`np.corrcoef(x,t)`로 찾을 수 있다.

산점도를 통해 상관관계를 시각적으로 파악할 수 있다.

`plt.scatter(x,t)`를 통해 산점도를 plot할 수 있다.

#### 모델

특징 변수들을 입력층(각각 하나의 뉴련), 가중치와 바이어스를 파라미터, 출력층을 예측 변수로 신경망 관점에서 선형회귀 모델을 나타낸다.

nn.Module을 상속받아 클래스를 만든 후 model 인스턴스를 만든다.

model의 함수들을 메서드라고 한다.

`__init__()`은 생성자로, `nn.Linear()` 등으로 파라미터 설정, `super(모델,self).__init__()`로 nn.module의 생성자를 받아 모든 초기화를 진행한다.

`forward()`함수는 순전파 연산을 정의하는 함수이다.

#### 학습

오차란 목표 변수와 예측 변수의 사이를 의미한다.

이러한 오차를 줄이는 w, b를 찾는 것이 학습의 목표이다.

단순히 오차를 손실 함수로 사용하면 정확하게 오차를 구하기 힘들기 때문에 MSE(평균 제곱 오차) 등의 오차를 사용함.

MSE는 오차를 제곱한 것의 평균이다. 제곱을 하면 convex해지는 효과도 추가로 얻을 수 있다.

$$ Loss Function(MSE) = \frac{1}{n}\sum_{i=1}^{n}[t_i -(wx_i+b)]^2$$

#### MAE

단순히 오차에 절댓값을 취해 평균을 취할 수도 있으며, 이상치에는 덜 민감하다.

$$ MAE = \frac{1}{n}\sum_{i=1}^{n}|t_i -(wx_i+b)|$$


### 경사하강법

머신러닝의 Optimization 알고리즘 중 하나로, 가중치와 바이어스의 최적값을 찾기 위해 사용

가중치 w값의 변화에 따른 손실 값을 보고, 경사(gradient)를 측정함.

(w,l(w,b))에서의 경사에 대한 수식 표현

Chain rule을 이용해,

$$ \frac{\partial l(w,b)}{\partial w} = \frac{\partial l(w,b)}{\partial y} \frac{\partial y}{\partial w} $$

로 두 기울기의 곱으로 값을 경사의 값을 구한다.

이때 b = 0이라고 가정하면

$$\frac{\partial l(w,b)}{\partial y} = \frac{1}{n}\cdot (-2) \sum_{i=1}^{n}(t_i-y_i) $$

$$ \frac{\partial y}{\partial w} = x_i$$

따라서 

$$ \frac{\partial l(w,b)}{\partial w} = \frac{1}{n}\cdot (-2) \sum_{i=1}^{n}(t_i-y_i) x_i $$
이다.

torch에서는 `loss.backward()`를 통해 자동으로 미분값을 계산해 준다.

#### 학습률

인공지능에서 매우 중요한 hyperparameter로, 가중치가 업데이트되는 크기를 결정한다.

결정법은 다양한 방법이 있지만, 시행착오를 거쳐가며 최적의 값을 찾아가야 한다.

학습률 $\alpha$를 포함한 가중치 업데이트 식

$$ w^{*} = w - \alpha \frac{\partial l(w,b)}{\partial w} $$


학습 과정은 일반적으로 다음과 같다.

```

optimizer.zero_grad() #이전 단계의 기울기를 초기화

predict = model(x) #예측값

loss = criterion(predict,y) #손실함수

loss.backward() #미분값을 계산

optimizer.step() #가중치 업데이트
```

#### 확률적 경사하강법

모든 데이터를 사용하여 w와 b를 업데이트하면, 계산 비용이 높아지고, 최솟값이 아니라 국소적인 최솟값으로 빠져 버리는local minima 문제가 생긴다.

따라서, 각각의 데이터 포인트마다 오차를 계산하여 w, b를 업데이트하는 알고리즘을 SGD(stochastic gradient descent, 확률적 경사하강법)이라고 한다.

$$ \frac{\partial l(w,b)}{\partial w} = \frac{1}{n}\cdot (-2) (t_i-y_i) x_i $$

각 데이터 포인트의 노이즈가 local minima에서 탈출을 하는 것을 도와준다.

```
import torch.optim as optim

optimizer = optim.SGD(model.parameters(),lr)
```
로 SGD optimizer를 불러올 수 있다.

#### 에폭

epoch은 한 trainig dataset을 완전히 학습하는 과정으로, 학습 과정에서는 하나의 hyperparameter가 된다.

epoch을 반복하면 모델의 성능이 향상될 수 있지만,

training dataset에 너무 맞춰지는 과적합(overfitting)이 발생함에 주의.


