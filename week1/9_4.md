# 9/4 - 과제 리뷰

## 텐서

자료형의 분석과 형변환(과 이에 따른 data 손실)에 대한 이해를 도와주는 부분이 있었다.

텐서의 생성: empty로 생성하면 빠르지만, 그 안의 값은 알 수 없다.

`fill_`를 사용하면 in-place로 값들을 채워 넣을 수 있다. -> 같은 id 주소 갖는 텐서를 return한다.

크기를 유지하고 싶을 때는 `_like`가 붙은 생성자를 사용해주면 보통 된다.

## CUDA

``` 
device = 'cuda' if torch.cuda.is_available() else cpu 
tensor.to(device)
```

이러한 식으로 코드를 짜놓으면, device 확인과 tensor 넘기기는 문제 없을 것 같다.
device로 넘겨야 할 데이터만 잘 생각하자.

## 데이터의 변환

데이터의 형태는 `view()`와 `reshape()`를 통해 변환 가능하다.
단, `view()`는 연속성이 보장되어야 사용 가능하며, `is_contiguous()`로 연속성을 확인할 수 있다.
`transpose()`는 두 전환과는 다르게, 차원 순서를 뒤집는다고 생각하면 쉬울 것 같다.

## 노이즈

random noise는 randn_like를 사용했는데,

gaussian noise의 경우에는 노이즈 값을 키워도 완전 랜덤을 보장하기는 어려워 보인다. 내 눈에는 원본 값을 찾기 힘들어 보이지만.. 수학적으로는 남은 정보량이 있을 것이다.

완전 랜덤으로 노이즈 이외의 정보를 얻지 못하게 하려면, uniform random 함수를 이용해야 할 것으로 보인다.

## 데이터의 학습 과정

학습 과정에서는 

```
optimizer.zero_grad()
outputs = model(inputs)
loss = criterion(outputs,targets)
loss.backward()
optimizer.step()
```
이 순서의 train 과정과, 
parameter의 구조(intercept가 뒤에 저장된다는 것 등)을 확인했다.


## ReLU

내가 짠 ReLU 코드가 맘에 들지 않아, 비교를 위해 `torch.nn.ReLU`를 들여다 보았다.

코드를 타고 올라가 보니 `torch.relu_`라는 함수가 나오는데. 보이지 않는다. C로 짜여진 low-level code인듯하다.

https://github.com/pytorch/pytorch/blob/28f43c767c0de48885f8de285ca4af4424d28f29/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu

ReLU 연산의 c 코드는 찾지 못했지만, pReLU와 leaky ReLU의 코드는 확인했는데,

더 복잡한 연산임에도 간단하게 핵심 코드 한 줄 외에는 타입처리 정도인 것을 보면 더 간단한 ReLU의 경우에도 그렇게 신기한 trick은 존재하지 않을 것이란 생각이 들었다.

## 내 코드 중 맘에 안들었던 것들..

손실함수 계산은 `scatter_()`와 `unsqueeze()`를 통해 0과 1로 one-hot encoding을 하였다.

이 과정에서, `zeros_like()`함수를 호출한 후 one-hot encoding 후, dot product를 구하는 방식으로 했는데,

뭔가 pythonic하지 않은 코드를 만든 것 같다.

train에서 loss를 더할 때 단순히 `+=loss`로 작성했는데, 여기서 문제가 한 번 발생했다.

loss를 더한 running loss에 require_grad가 붙어 추적이 되는 상태가 되어, 오류가 발생했다.

이를 단순히 `.detach()`를 붙혀 주는 것으로 해결했는데, 쓸모없는 그래디언트 추적에 들어가는 연산을 줄이기 위해서는

loss 대신 `loss.item()`으로 scalar 형태로 더해 주는 것이 필요하겠다. (nn을 사용한 곳에서는`item()`으로 잘 사용하였다)


## Trick?

나는 `logsumexp()` 함수를 써서 몰랐는데, Cross Entropy를 구하는 도중에 값이 NaN으로 튀는 현상이 있는 모양이다.

조형동 팀원님이 올려준 LogSumExp trick을 보고 계산 시 크기를 조절하는 방안을 보았다.

https://github.com/pytorch/pytorch/blob/01d98c7cfb2031fd5ab4148444a2d34a171e700c/aten/src/ATen/native/ReduceOps.cpp#L333


```
Tensor& logsumexp_out(Tensor& result, const Tensor& self, IntArrayRef dims, bool keepdim) {
  // can't take max of empty tensor
  if (self.numel() != 0) {
    auto maxes = at::max_values(self, dims, true);
    auto maxes_squeezed = (keepdim ? maxes : squeeze_multiple(maxes, dims));
    maxes_squeezed.masked_fill_(maxes_squeezed.abs() == INFINITY, 0);
    at::sum_out(result, at::exp(self - maxes), dims, keepdim);
    result.log_().add_(maxes_squeezed);
  } else {
    at::sum_out(result, at::exp(self), dims, keepdim);
    result.log_();
  }
  return result;
}
```

`torch.logsumexp()` 함수 또한 문제를 일으키지 않아서 확인을 위해 torch의 logsumexp 구현체를 보니, LogSumExp trick이 구현되어 있는 것을 확인하였다.



#### (피어 세션) TODO- "양자화된" int

floating point의 고정 소수점보다, 정수형 자료형보다 훨씬 무겁기 때문에 rounding을 통해 양자화해서 사용하는 데이터 타입인

`qint`자료형이 존재한다.

다만, rounding 과정에서 자동미분이 불가능해지기 때문에 자동미분이 필요없는 inference 등에 사용된다.

#### 피어 세션: view와 reshape

`view()`가 바로 다음 메모리 주소로 연결된 텐서가 아니더라도, 일정한 stride를 가진 텐서에 대해서는 정상 작동한다는 것을 확인했다.

contiguous 속성을 말 그대로 '주소가 연속적인' 것보다는, '어떤 stride로 보았을 때에는' 연속적이다..라고 이해하면 될 것 같다.
