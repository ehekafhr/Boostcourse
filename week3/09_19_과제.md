# Weekly mission

정말.. 오래걸린다. 한 에폭당 T4 GPU로 15분정도 걸리는 것 같다.

사실 그렇게 큰 모델도 아니다. N=3짜리, 논문에서 나온 것보다 작은 모델이다.

Transformer를 만들어 보는 mission이었는데, mission에 masking 부분이 빠져 있어 제대로 학습이 안 된 것 같다..

왜인지는 모르겠는데, decoder 단이 항상 "eos"를 내뱉어서 문장이 바로 끝난다.
Training할 때는 문제를 안 일으키는 걸로 보아.. start token을 인식하지 못하게 학습되는 것 같다.

Colab의 GPU 사용량 제한을 넘기게 하는 과제. 한 번에 성공 못한다면.. CPU로 넘어가는데 코랩의 cpu를 쓸 바에는 로컬로 돌리는 게 낫다.

아무튼 과제 시간이 끝나고, masking을 제대로 하도록 코드를 고친 결과.. 어떻게든 말은 하는 모델이 나왔다.

<img width="1120" height="101" alt="image" src="https://github.com/user-attachments/assets/44fdf8cf-bc9c-4527-a474-6b8a4c744420" />

아마 Transformer 단이 짧기 때문에, 충분히 다음 단어로 이어지지 않아서 같은 단어를 반복하는 현상이 자주 나왔다. N을 키우고 학습을 많이 하면 이런 현상이 사라질 것으로 보인다.

BLEU score라는 개념이 나왔는데, 식은 다음과 같다

n-gram 보정된 정밀도

$Count_{clip}$ 은 총 n-그램 중에서, Ref에서 등장한 횟수를 max로 cliping 해 준 값이다!

$$ p_n = \frac{\sum_{n-gram \in Candidate }{Count_{clip}(n-gram)}}{\sum_{n-gram \in Candidate }{Count_{clip}(n-gram)} $$

즉, n-gram이 얼마나 겹치느냐를 사용한다.(연속된 n개으 단어)

이를 모두 더해 주어 제곱한 스코어에다가, 브레버티 페널티 BP를 곱해 준다(문장의 길이가 짧은 경우 점수가 높게 나오므로, Ref보다 길이가 짧은 문장에 페널티)

$$ BELU = BP \times exp\sum_{n=1}^{N}{w_n log p_n} $$

가 된다.

한 10쯤 나왔는데.. 20이 나와야 하는 걸 보면 모델의 크기가 작고, 데이터도 작으니 학습이 잘 안 되는 것 같다..

그래도 간단한 문장은 나왔다.

<img width="515" height="94" alt="image" src="https://github.com/user-attachments/assets/61c1090a-8caa-48e1-854f-a9b40e31ee25" />

인사는 잘 받아준다!
