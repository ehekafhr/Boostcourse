# 정리

이번 주 강의의 초반부분은 머신러닝에 대한 intro와,
이떄까지 배운 부분을 다시 되짚어보며, 머신러닝 관점에서 해석해보는 부분으로 보인다.

## 머신러닝

머신러닝은 인공지능에 포함되고, 딥러닝을 포함하는 개념으로,

Tom Mitchell의 정의에 따르면 머신러닝은

$$ 작업 T를 경험 E를 통해 성능 P를 향상시키는 알고리즘 $$

이다. 일반적으로 작업은 우리의 목적(분류, 회귀, 군집 등), 경험 E는 데이터셋과 이를 통한 학습이고,

정확도는 이러한 작업이 얼마나 잘 이루어졌는지를 의미하게 된다.

머신러닝의 특징으로는, "프로그램"을 찾아나가는 과정이라는 것이 있다.

전통적인 과정에서는 데이터와 프로그램을 통해 출력값을 만든다면,

머신러닝에서는 데이터와 출력값을 통해 프로그램을 학습시킨다.

물론, 머신러닝을 통해 만든 프로그램의 사용은 전통적 프로그래밍과 같이 데이터와 프로그램으로 출력값을 뱉는다.

머신러닝에는 지도학습, 비지도학습, 강화학습이 있다.

지도학습은 label, 즉 정답이 존재하여 이를 통해 학습을 진행한다.

*추가: semi-지도학습이라는 것이 존재한다. 적은 수의 labeling된 데이터를 통해, labeling된 데이터를 늘려가며 지도학습처럼 동작하는 알고리즘이다.

비지도학습은 label이 없는데, 군집화 등을 사용하여 데이터의 특성을 추출한다.

강화학습은 "Agent"를 학습시킨다는 개념으로, 실제 환경과 같은 시뮬레이션을 진행하며 Agent가 최고의 성능을 내도록 발전시키는 방식이다.

## 머신러닝 라이프사이클

사이클의 구성:

### 1.계획(Plan)

어떤 모델을 할 것인지, 어떤 평가 지표를 사용할 것인지, 실현 가능성은 얼마나 되는지를 평가하고 계획하는 단계이다.

### 2.데이터 준비

데이터를 수집-라벨링, 정리, 처리, 관리하는 과정이다.

이상치를 제거하고, 적절한 처리를 거쳐 학습하기 쉬운 데이터 형태로 바꾸는 단계이다.

### 3.모델 엔지니어링

모델의 아키텍쳐-어떤 모델을 설정할지, 파라미터는 어떻게 할지 등을 정하고, 파이프라이닝을 통해 모델을 학습시킨다.

모델의 결과를 해석하는 것까지 포함한다.

### 4.모델 평가

"테스트 데이터셋"으로 모델을 테스트해보고, 전문가와 함께 오류를 파악하는 단계이다.

또한 이렇게 만들어진 모델이 사회적으로 문제가 없는지(윤리적으로 편향되어 있을 수도 있고, 법적으로 사용 불가능한 모델일 수도 있다), 이상치에 민감한지(robust한지) 확인하여 배포 여부를 결정한다.

### 5.모델 배포, 유지관리

API, 웹 등을 통하여 접근 가능하도록 머신러닝 모델을 배포하고,

지속적으로 들어오는 데이터에 대한 지표와 만족도 지표를 통해 모델을 개선한다.

## 선형회귀

### 어원 
프랜시스 골턴의 이야기에서 "회귀"라는 용어가 나왔는데,

여기서의 "회귀"의 의미는 조금 다른 것 같다.

상관관계를 의미하는 "회귀"가 아니라, 어떤 전체 평균으로 돌아간다는 "회귀"인데, 그냥 쓰는 것 같다..

### 용어

독립변수: 결과의 원인(notation: x)

독립변수라는 이름이 붙은 이유는 이 변수들이 독립이라고 가정하기 때문으로 보인다. 그렇지 않다면, 다중공선성 문제가 발생한다.

종속변수: 결과(notation: y)

일반적으로 우리가 사용하는 라벨 등의 결과값이다.

### 가정

선형성: 종속 변수와 독립 변수 간 선형적 관계가 존재한다. $ y = mx + b $

독립성: 잔차들이 무작위로 분포되어야 한다.

등분산성: 잔차들이 일정 분포를 따라야 한다.

정규성: 잔차가 정규 분포를 따른다.

정규성이 만족되면 독립성, 등분산성은 다 되는 것 아닌가..?

### OLS(최소제곱법)

관측값(y)와 모델의 predict 값을 최소화하는 매개변수를 추정.( $ m, b $ )

다차원임에 주의.

### 지표

MSE(metrics.mean_squared_error):

$$MSE =\frac{1}{n}\sum_{i=1}^{n}{(y_i-\hat{y_i})^2}$$

RMSE(MSE에 np.root)

$$RMSE = \sqrt {\frac{1}{n}\sum_{i=1}^{n}{(y_i-\hat{y_i})^2}}$$

MAE(metrics.mean_absolute_error):

$$ MAE =\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y_i}| $$

R-square(metrics.r2_square): 1에 가까울수록 설명 잘함, 0인 경우는 그냥 평균값 때리는 수준이라는 것. 음수면 그보다 못하다.

$$ 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y_i})^2} $$

$$ SST = (y_i-\bar{y_i})^2, \ 종속\ 변수의\ 변동성을\ 의미 \\   SSR = (\hat{y_i}-\bar{y_i})^2, \ 모델에\ 의해\ 설명되는\ 변동성 \\  
SSE = (y_i-\hat{y_i})^2, \ 오차값 \\    R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST} $$


## k-NN

단순히 라벨된 데이터 포인트들을 가지고, "가장 가까운" 라벨된 포인트들이 투표를 해서 가장 많은 포인트들의 라벨로 라벨링하는 방법.

"lazy"한 방법으로, 이해하기도, 구현하기도 쉽지만 정말 좋지 않은 방법이다.

"학습" 과정이 없다.. label-point 쌍만 있으면 끝.

문제는 저 쌍 만큼의 iteration을 돌아야 한다는 것. 매우 느리다.

특히, 고차원 data의 경우에는 포인트가 엄청나게 필요해서 불가능한 수준이다.

## 분류기

분류 문제는 입력을 특정 클래스로 분류하는 함수이다.

선형회귀와 똑같이 $ mx + b $ 를 계산할 수 있지만, 해석에 유의해야 한다.

### Softmax

우리가 $ mx + b $ 를 통과한 값을 이해하는 어렵다. 최솟값도, 최댓값도, 평균도 없다!

그렇기 때문에, 이를 특정한 조건을 만족시키는 값으로 보내서 "확률"로 해석하기 위한 함수가 softmax 함수이다.

조건: 

$ 1) mx + b $ 를 통과한 점수가 더 큰 값이 더 큰 확률을 갖는다.

$ 2) $ 확률의 합은 1이다.

이 두 가지를 만족하는, Softmax function

$$ p(y=c_i|\textbf{x}) = \frac{e^{s_i}}{\sum_j{e^{s_j}}} $$

가 된다. 이때 주의할 점은, 실제 계산은 분모와 분자를

$$ max(e^{s_j}) $$ 

로 나누어 준다는 것이다.(계산의 안정성을 위해)

### 손실함수

회귀 문제와 다르게, MAE나 MSE 등의 손실함수를 사용하기는 어렵다.

#### 마진 기반 손실

잘 사용하지는 않지만, 알아 두자..

predict $ \hat{y} $ 와 실제 값  $ y \in {1,-1} $ 에 대해,

손실을 $ y\hat{y} $ 를 통해 결정한다. 부호가 같으면 손실이 작아지고, 부호가 다르면 손실이 커진다.

쓴다면 힌지 손실(계산 효율), 로그 손실(해석의 편리함)을 쓴다.

##### step function

$ y\hat{y} $ 에 step function을 적용. 잘못 분류하면 손실 1, 아니면 0인데.. "미분 불가능"

##### log 손실

$ log(1+y\hat{y}) $ 를 사용. 미분 가능하다.

##### 지수 손실

$ e^{-y\hat{y}} $ 를 사용. 미분 가능하다. 로그보다 좀 더 엄격하다. "폭발 가능"

##### Hinge 손실

$ max(0,1-y\hat{y}) $ 를 사용. 미분 가능하다. 계산도 쉽고(ReLU 같다..)..

#### Cross Entropy

식이 복잡하지만, 결국 $ y $를 one-hot vector로 본다면 y가 1인 것만 살아남을 것이므로..

$$ Loss = -\frac{1}{N} \sum_{i=1}^{N}{log(\hat{y_i}T_i)} $$

로 마이너스 로그의 평균이 된다!

이때, 확률이 0에 가까우면 손실이 너무 커질 수 있으니 주의하자..(-log x 함수에서, [0,1] 사이의 값이므로 최댓값은 없고 최솟값 0을 가진다)

### 최적화

"특정 제약" 하에서 최고의 값을 찾는 문제.

완전 탐색, 랜덤 탐색, 시각화 등의 방법이 있지만.. 인공지능에서는 Gradient Descent 방법을 사용한다.

#### Gradient Descent

이전 강의들에서 본 Gradient Descent를 다시 설명했다. 이전 자료를 참고하자. 마무리 기준을 정하는 부분만 추가된 것 같다.(변화율이 일정 미만이면 stop)

# 과제 1번

pandas의 indexing을 알아야 풀 수 있는 문제였다.. iloc등을 활용한 indexing 필수.

선형 회귀의 방법 뿐만 아니라 한계점을 알 수 있는 과제였다. 

선형 회귀의 조건 중 독립성을 만족하지 않는 data인 것 같다. 이런 홍합같은?데이터의 경우에는 선형회귀를 적용하기 힘들다는 것을 보여준다.
