# 강의정리

## Attention

RNN-based model은 어떻게 LSTM 등으로 Long-Term memory를 저장하든,

"긴 sequence"에서 앞의 정보가 소실되는 것을 방지할 수 없다.

따라서, input step의 "모든 hidden state"를 고려하는 방식 사용.

## Transformer

## Bert

## ViT
