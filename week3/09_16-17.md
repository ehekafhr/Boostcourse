# 강의정리: 신경망

선형 모델은 하나의 클래스 당 하나의 템플릿만 학습 가능하다.

또한, 바운더리가 선형적이라는 문제가 있다. 

좌표계 변환을 이용해 입력을 "선형 분류 가능한" space로 보내 준다면 선형 모델로도 충분하겠지만..

가능하다는 보장도 없다.


## 신경망

뉴런의 형태를 모사한 것으로, 뉴런 -> 시냅스 -> activation -> output을 모사했다.

일반적으로 Layer -> Activtion function -> Layer ... -> output 이런 식으로

Multi-layer를 쌓는다.

이때, Activation function은 "비선형성"을 주어 하나의 레이어를 쌓는 것과 다른 결과를 낳게 해 준다.

## Backpropagation

경사하강법은 chain rule을 통해, loss에서부터 한 단계씩 뒤로 가면서 전파를 해 나간다.

$$ \frac{\partial L}{\partial w} = \frac{\partial z}{\partial w} \frac{\partial L}{\partial z} $$

그리고 그 다음 단 w2는

$$ \frac{\partial L}{\partial w2} = \frac{\partial w}{\partial w2} \frac{\partial L}{\partial w2} $$

이런 식으로 뒤로뒤로 간다.

덧셈의 경우에는 gradient를 더해지는 값들에 그대로,

곱셈의 경우에는 반대편 입력 값을 곱하고,

max의 경우에는 큰 쪽에만 gradient를 보내 준다.

softmax함수의 경우의 미분은 $\hat{y}-y$ 로 간단하게 표시되어 계산하기 쉽다.

sigmoid의 경우는 $y(1-y)$ 로..

## Activation Functions

### Sigmoid

$$ \sigma (x) = \frac{1}{1+e^{-x}} $$

출력 값을 확률로 해석 가능!

하지만 0보다 큰 증가함수이기 때문에.. 그래디언트의 부호가 변하지 않는다.

Vanishing gradient가 발생한다. (값이 너무 크면? 미분값이 너무 작아진다)

### Tanh

$$ tanh(x) = \frac{e^x - e^{-x}}{e^x+e^{-x}} \ $$

Zero-centered (-1,1) 범위.

똑같이 Vanishing gradient 발생

### ReLU

$$ ReLU(x) = max(0,x) $$

값이 커져도 미분값은 1이다! 참 좋다. 보통 이 계열을 사용한다.

계산도 쉽다.

#### leaky ReLU

$$ lReLU(x) = x if x>= 0, \alpha x if x<0 $$

ReLU 뉴런 초기화에 도움을 준다.

#### ELU

$$ ELU(x) = x if x>= 0, \alpha (e^x-1) if x<0 $$

ReLU계열의 x=0일때 미분 불가능 해결 but 지수계산은 비싸다..

## Weight Init

### Gaussian Random

Weight 초기화. 하지만 그 분산값을 어떻게 설정할 것인가?

Xaiver에서는 in과 out dimension을 더해서 분모로..

$$ Var(W) = \sqrt{\frac{2}{in+out}} $$ 

인 정규분포 혹은

$$ (-\sqrt{\frac{6}{in+out}}, \sqrt{\frac{6}{in+out}}) $$ 

인 uniform 분포를 따르게 초기화한다. 이는 sigmoid, tanh를 activation function으로 사용할 때이고,

He에서는 out이 빠진 값이다.

$$ Var(W) = \sqrt{\frac{2}{in}} $$ 

인 정규분포

$$ (-\sqrt{\frac{6}{in}}, \sqrt{\frac{6}{in}}) $$ 

인 uniform 분포를 따르게 한다.

## Learning Rate Scheduling

[이전 정리 노트에서 ](https://github.com/ehekafhr/Boostcourse/blob/main/week2/9_09.md) 정리했던 내용을 다시 보면 될 것 같다!

## Preprocessing & Augementation

$$ \ket{\psi} = cos(\frac{\theta}{2}) \ket{0} + e^{i \phi}sin(\frac{\theta}{2}) \ket{1} $$ 
