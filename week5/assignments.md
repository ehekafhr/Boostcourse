# assignment 1

단순한 단어 단위 토큰화 과제. Vocabulary의 size 문제를 해결하기 위해 빈도(Frequency)를 사용해 cut했다.

한국어는 라이브러리를 가져와서 토크나이저를 사용하는 것만 해 보았다.

한국어의 경우에는 띄어쓰기 단위로 자를 경우 조사가 증발하는 문제가 있기 때문에.. 조사를 따로 떼어주는 것 같다.

조사는 영어로 쓰면 굉장히 지루하고 현학적인 단어가 되어버리기 때문에 그냥 Josa라고 한 것 같다. 푸하하

# assignment 2

Subword 단위 토큰화. 쉽지않다.

BPE 방식으로 iterative하게 하나씩 추가하는데, 정규 표현식에 익숙하지 않아 괜한 if문이 많아진 것 같다.

vocab을 만들 때에는 re를 이용한 pattern으로 단어를 합치고 찾고 하는 것이 쉽지 않았고,

tokenization에서는

"가장 긴 subword" 부터 잘라내어 남은 부분을 tokenize할 때 순서 유지를 위해 위치를 기억하고, 자른 subword가 맨 앞이나 맨 끝일 경우를 따로 처리해 주어야 했기 때문에 괜한 if문이 많아진 것 같다.

# advanced

Word2Vec의 두 방법론인 skip-gram과 CBoW 방법론을 적용해 학습시켜 보는 과제였다.

가생이에 있는 단어를 처리하기 위해 고민 중, skip-gram의 경우에는 y를 one-hot vector끼리 더해 주는 형식으로 해결하였고, CBoW의 경우에는 그렇게 하기 힘들어 Padding을 의미하는 Token을 추가해 주고, Embedding 시에 그것을 표시해 학습되지 않게 해 주었다.

PCA 등으로 시각화했을 때 무언가 비슷한 단어들끼리 모여 있는 것 같으면서도 애매한 모습이었는데, 학습에 사용된 문장이 10개의 문장밖에 되지 않았기 때문으로 추정된다.
